{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import copy\n",
    "from operator import itemgetter\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Chapter7.PrepareDatasetForLearning import PrepareDatasetForLearning\n",
    "from Chapter7.Evaluation import ClassificationEvaluation\n",
    "from Chapter7.Evaluation import RegressionEvaluation\n",
    "from Chapter7.LearningAlgorithms import ClassificationAlgorithms\n",
    "from Chapter7.LearningAlgorithms import RegressionAlgorithms\n",
    "\n",
    "from util.VisualizeDataset import VisualizeDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset_imputed_values.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency domain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n"
     ]
    }
   ],
   "source": [
    "milliseconds_per_instance = (1/50)*1000\n",
    "print(milliseconds_per_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split for computation\n",
    "To optimally use our resources the trials per individual will be analysed independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chapter4.FrequencyAbstraction import FourierTransformation\n",
    "\n",
    "\n",
    "FreqAbs = FourierTransformation()\n",
    "fs = float(1000)/milliseconds_per_instance\n",
    "\n",
    "periodic_predictor_cols = list(dataset.columns[:12])\n",
    "\n",
    "\n",
    "average_t_per_100_rows = []\n",
    "\n",
    "def do_freq_abstract_for_trial_participant_and_save(trial, participant):\n",
    "    \"\"\"\n",
    "    Does freq abstrac on a single participant for a single trial. After inference results\n",
    "    are saved to a individual csv for later fusing results.\n",
    "    \"\"\"\n",
    "    \n",
    "    t_start = time.time()\n",
    "    \n",
    "    print(f'Worker doing trial {trial} and participant {participant}')\n",
    "    \n",
    "    ds = dataset[dataset.trial.eq(trial)][dataset.id.eq(participant)]\n",
    "    \n",
    "    print(f'Working on dataset of shape {ds.shape}')\n",
    "    \n",
    "    try:\n",
    "        expected_run_time = sum(average_t_per_100_rows)/len(average_t_per_100_rows) * (ds.shape[0]/100)\n",
    "    except:\n",
    "         expected_run_time = 'UNKNOWN'\n",
    "    \n",
    "    print(f'Expected run time: {expected_run_time} s')\n",
    "    \n",
    "    my_set = FreqAbs.abstract_frequency(ds, periodic_predictor_cols, int(float(10000)/milliseconds_per_instance), fs)\n",
    "    \n",
    "    my_set.to_csv(f'freq_abstraction_trial_{trial}_participant_{participant}.csv')\n",
    "    t_done = time.time()\n",
    "    total_time = t_done-t_start\n",
    "    print(f'Took {total_time} seconds')\n",
    "    run_time_per_row = total_time/ds.shape[0]\n",
    "    average_t_per_100_rows.append(100*run_time_per_row)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cormac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trials = dataset.trial.unique()[:5]\n",
    "participants = dataset.id.unique()\n",
    "\n",
    "with Pool(processes=cpu_count()) as p:\n",
    "    r = p.starmap(do_freq_abstract_for_trial_participant_and_save, itertools.product(my_trials, participants))\n",
    "    print(r)\n",
    "\n",
    "print('All done now')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker doing trial 1.0 and participant 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abel/Desktop/ai/AIMASTER/MLQS/ML4QS/env/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on dataset of shape (141, 41)\n",
      "Expected run time: UNKNOWN s\n",
      "Took 1.9797260761260986 seconds\n",
      "Worker doing trial 1.0 and participant 1.0\n",
      "Working on dataset of shape (172, 41)\n",
      "Expected run time: 2.414985000664461 s\n",
      "Took 1.9026076793670654 seconds\n",
      "All done now\n"
     ]
    }
   ],
   "source": [
    "my_trials = dataset.trial[5:10]\n",
    "participants = dataset.id.unique()\n",
    "\n",
    "with Pool(processes=cpu_count()) as p:\n",
    "    r = p.starmap(do_freq_abstract_for_trial_participant_and_save, itertools.product(my_trials, participants))\n",
    "\n",
    "print('All done now')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natasja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trials = dataset.trial[10:15]\n",
    "participants = dataset.id.unique()\n",
    "\n",
    "with Pool(processes=cpu_count()) as p:\n",
    "    r = p.starmap(do_freq_abstract_for_trial_participant_and_save, itertools.product(my_trials, participants))\n",
    "    print(r)\n",
    "\n",
    "print('All done now')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for csv_file_freq_abs in glob.glob('freq_abstraction_trial_*_participant_*.csv'):\n",
    "    datasets.append(pd.read_csv(csv_file_freq_abs))\n",
    "\n",
    "    \n",
    "    \n",
    "dataset = pd.concat(datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform selection on 20% of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Train split for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.sample(frac=0.2,random_state=200)\n",
    "train_x = train.drop(columns=['act'])\n",
    "train_y = train['act']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " selected_features, ordered_features, ordered_scores = FeatureSelectionClassification().forward_selection(max_features, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.to_csv(Path('selected_set.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments - whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(Path('selected_set.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with random forest across all targets. Whatever target seems to be most predictable we will apply other algorithms to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict activity label - whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take random samples for this experiment\n",
    "train=dataset.sample(frac=0.3) #random state is a seed value\n",
    "test=dataset.drop(train.index)\n",
    "test= test.sample(frac=0.3)\n",
    "\n",
    "train_y = train['act']\n",
    "train_X = train.drop(columns=['act'])\n",
    "test_y = test['act']\n",
    "test_X = test.drop(columns=['act'])\n",
    "\n",
    "pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y = ClassificationAlgorithms().random_forest(train_X, train_y, test_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict age based on sensor values - whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dataset.sample(frac=0.3)\n",
    "test=dataset.drop(train.index)\n",
    "test= test.sample(frac=0.3)\n",
    "\n",
    "train_y = train['age']\n",
    "train_X = train.drop(columns=['age'])\n",
    "test_y = test['age']\n",
    "test_X = test.drop(columns=['age'])\n",
    "\n",
    "return pred_training_y, pred_test_y = RegressionAlgorithms().random_forest(self, train_X, train_y, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict gender - whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dataset.sample(frac=0.3)\n",
    "test=dataset.drop(train.index)\n",
    "test= test.sample(frac=0.3)\n",
    "\n",
    "train_y = train['gender']\n",
    "train_X = train.drop(columns=['gender'])\n",
    "test_y = test['gender']\n",
    "test_X = test.drop(columns=['gender'])\n",
    "\n",
    "pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y = ClassificationAlgorithms().random_forest(train_X, train_y, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict weight - whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dataset.sample(frac=0.3)\n",
    "test=dataset.drop(train.index)\n",
    "test= test.sample(frac=0.3)\n",
    "\n",
    "train_y = train['weight']\n",
    "train_X = train.drop(columns=['weight'])\n",
    "test_y = test['weight']\n",
    "test_X = test.drop(columns=['weight'])\n",
    "\n",
    "\n",
    "return pred_training_y, pred_test_y = RegressionAlgorithms().random_forest(self, train_X, train_y, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments - By Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = [dataset[dataset['trial']==i] for i in dataset['trial'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_alg = ClassificationAlgorithms()\n",
    "\n",
    "def generate_sets(data_set):\n",
    "    train=dataset.sample(frac=0.3) #random state is a seed value\n",
    "    test=dataset.drop(train.index)\n",
    "    test= test.sample(frac=0.3)\n",
    "\n",
    "    train_y = train['act']\n",
    "    train_X = train.drop(columns=['act'])\n",
    "    test_y = test['act']\n",
    "    test_X = test.drop(columns=['act'])\n",
    "    \n",
    "    return train_y, train_X, test_y, test_X\n",
    "\n",
    "\n",
    "\n",
    "def train_classification(train_y, train_X, test_y, test_X, class_alg):\n",
    "    pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y = class_alg.random_forest(train_X, train_y, test_X)\n",
    "    return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "def evaluate_classifcation(y_true, y_pred):\n",
    "    evaluation = ClassificationEvaluation()\n",
    "    return evaluation.accuracy(y_true, y_pred), evaluation.precision(y_true, y_pred), evaluation.recall(y_true, y_pred), evaluation.f1(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest for the labels by trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "prec = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for subset in trials:\n",
    "    train_y, train_X, test_y, test_X = generate_sets(subset)\n",
    "    pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y = train_classification(train_y, train_X, test_y, test_X, class_alg)\n",
    "    accuracy, precision, recall, f1_ = evaluate_classifcation(test_y, pred_test_y)\n",
    "    acc.append(accuracy)\n",
    "    prec.append(precision)\n",
    "    recall.append(recall)\n",
    "    f1.append(f1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
