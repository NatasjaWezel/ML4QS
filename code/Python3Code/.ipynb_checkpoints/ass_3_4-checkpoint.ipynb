{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cb67e74855d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mviz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualizeDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Question 4 of chapter 3:\n",
    "Similarly to what we have done for our crowdsignals dataset,apply the techniques\n",
    "that have been discussed in this chapter to the dataset you have collected yourself.\n",
    "Write down your observations and argue for certain choices you have made.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from Chapter3.OutlierDetection import DistributionBasedOutlierDetection\n",
    "from Chapter3.ImputationMissingValues import ImputationMissingValues\n",
    "from Chapter3.KalmanFilters import KalmanFilters\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load the granularized dataset\n",
    "\"\"\"\n",
    "\n",
    "DATASET_DIR = Path('./intermediate_datafiles/chapt2/')\n",
    "SMALL_GRAN_FILENAME = '250_ms_gran.csv'\n",
    "LARGE_GRAN_FILENAME = '1000_ms_gran.csv'\n",
    "dataset_small = pd.read_csv(\n",
    "    Path(DATASET_DIR / SMALL_GRAN_FILENAME), index_col=0)\n",
    "dataset_small.index = pd.to_datetime(dataset_small.index)\n",
    "\n",
    "dataset_large = pd.read_csv(\n",
    "    Path(DATASET_DIR / LARGE_GRAN_FILENAME), index_col=0)\n",
    "dataset_large.index = pd.to_datetime(dataset_large.index)\n",
    "\n",
    "DATASETS = {'250_ms_gran': dataset_small, '1000_ms_gran': dataset_large}\n",
    "DATA_COLUMNS = (\n",
    "    'acceleration_X', 'acceleration_Y', 'acceleration_Z', 'Illuminance',\n",
    "    'accelerationlinear_X', 'accelerationlinear_Y', 'accelerationlinear_Z',\n",
    "    'compass_X', 'compass_Y', 'compass_Z', 'Latitude', 'Longitude',\n",
    "    'Altitude', 'gravity_X', 'gravity_Y', 'gravity_Z', 'gyro_X',\n",
    "    'gyro_Y', 'gyro_Z', 'rotation_X', 'rotation_Y', 'rotation_Z',\n",
    "    'rotation_cos', 'rotation_headingAccuracy'\n",
    ")\n",
    "\n",
    "viz = VisualizeDataset(__file__)\n",
    "\n",
    "\"\"\"\n",
    "Lets first analyse the distribution of the measured variables\n",
    "\n",
    "for ds in DATASETS:\n",
    "    for col in DATA_COLUMNS:\n",
    "        print(DATASETS[ds][col].shape)\n",
    "\n",
    "        plt.hist(DATASETS[ds][col], bins=40)\n",
    "\n",
    "        plt.title(f'Distribution of data {col} for gran {ds}')\n",
    "\n",
    "        plt.show()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Lets choose the measurement acceleration_X\n",
    "and process the 250ms as well as the 1000 ms aggregation.\n",
    "First we want to remove outliers.\n",
    "\"\"\"\n",
    "col = 'acceleration_X'\n",
    "\n",
    "\"\"\"\n",
    "Lets see how the col is distributed roughly\n",
    "\"\"\"\n",
    "for ds in DATASETS:\n",
    "    # plt.hist(DATASETS[ds][col], bins=40)\n",
    "    # plt.title(f'Distribution of data {col} for gran {ds}')\n",
    "    # plt.show()\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Lets analyse the outliers under the assumption the set is normal distributed\n",
    "\"\"\"\n",
    "\n",
    "outlier_detection = DistributionBasedOutlierDetection(mixture_comp=2)\n",
    "for ds in DATASETS:\n",
    "    dataset = DATASETS[ds]\n",
    "    dataset = outlier_detection.chauvenet(dataset, col)\n",
    "    # viz.plot_binary_outliers(dataset, col, col + '_outlier')\n",
    "\n",
    "\"\"\"\n",
    "acceleration_X might be a mixture model of two gaussian distributions\n",
    "Lets look at the likelihood of the observed measurements under this assumption:\n",
    "\"\"\"\n",
    "\n",
    "for ds in DATASETS:\n",
    "    dataset = DATASETS[ds]\n",
    "    dataset = outlier_detection.mixture_model(dataset, col)\n",
    "    # viz.plot_dataset(\n",
    "    #    dataset, [col, col + '_mixture'], ['exact', 'exact'], ['line', 'points'])\n",
    "\n",
    "\"\"\"\n",
    "Lets apply chauv criterion to all cols for the two granularities\n",
    "and save them as interm result\n",
    "\"\"\"\n",
    "for ds in DATASETS:\n",
    "    dataset = DATASETS[ds]\n",
    "    for c in dataset.columns:\n",
    "        if '_outlier' in c or '_mixture' in c or 'lof' in c or 'simple_dist_outlier' in c:\n",
    "            if c in dataset.columns:\n",
    "                dataset.pop(c)\n",
    "            continue\n",
    "        dataset = outlier_detection.chauvenet(dataset, c)\n",
    "        dataset.loc[dataset[f'{c}_outlier'] == True, c] = np.nan\n",
    "        del dataset[c + '_outlier']\n",
    "    dataset.to_csv(DATASET_DIR / (ds+'_outliers.csv'))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Now impute missing values\n",
    "\"\"\"\n",
    "# change dataset to be the outlier data set\n",
    "dataset_small = pd.read_csv(\n",
    "    Path(DATASET_DIR / '250_ms_gran_outliers.csv'), index_col=0)\n",
    "dataset_small.index = pd.to_datetime(dataset_small.index)\n",
    "\n",
    "dataset_large = pd.read_csv(\n",
    "    Path(DATASET_DIR / '1000_ms_gran_outliers.csv'), index_col=0)\n",
    "dataset_large.index = pd.to_datetime(dataset_large.index)\n",
    "\n",
    "\n",
    "for ds in DATASETS:\n",
    "    dataset = DATASETS[ds]\n",
    "    for c in dataset.columns:\n",
    "        MisVal = ImputationMissingValues()\n",
    "        imputed_mean_dataset = MisVal.impute_mean(\n",
    "            deepcopy(dataset), c)\n",
    "        imputed_median_dataset = MisVal.impute_median(\n",
    "            deepcopy(dataset), c)\n",
    "        imputed_interpolation_dataset = MisVal.impute_interpolate(\n",
    "            deepcopy(dataset), c)\n",
    "        viz.plot_imputed_values(dataset, ['original', 'mean', 'interpolation'], c,\n",
    "                                imputed_mean_dataset[c], imputed_interpolation_dataset[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
